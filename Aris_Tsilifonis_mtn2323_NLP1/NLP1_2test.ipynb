{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Aris Tsilifonis - mtn2323 - NLP Assignment 1 part 2"
      ],
      "metadata": {
        "id": "fFG6HHP_SI3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The second part of the project was to produce n-gram models than can generate sentences. They utilized bigrams and trigrams as well as add-k smoothing to find patterns in the text. The performance of those models was measured by their perplexity. Based on the probability distribution learned by the train set, the algorithm attempts to predict the following word in a sentence."
      ],
      "metadata": {
        "id": "o8M8Ui32n9AZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28vwq0EDUQrz",
        "outputId": "e75061f6-1e04-4c46-ea83-81a7a6b2a819"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "import random\n",
        "import math\n",
        "from nltk.util import ngrams\n",
        "from collections import defaultdict, Counter\n",
        "from nltk.corpus import treebank\n",
        "import re\n",
        "nltk.download(\"treebank\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download dataset, form train and test set\n",
        "Download treebank corpus from nltk library. Split corpus into train and test dataset"
      ],
      "metadata": {
        "id": "i3NaVUAbc93F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = treebank.fileids()\n",
        "train_documents = documents[:170] # 170 news files in train set\n",
        "test_documents = documents[170:] # 29 remaining news files in test set"
      ],
      "metadata": {
        "id": "cH7Ywc5BXWZy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create dataset\n",
        " Three version of the corpus will be created. The first one\n",
        "will contain both lowercased and capital characters. The second one will\n",
        "consist of lowercased characters only. The last one, which is called abstract\n",
        "digits, will replace all of the numerical characters with \\# symbol."
      ],
      "metadata": {
        "id": "MKjKp1aNdbZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the symbol to replace digits with\n",
        "digit_replacement_symbol = \"#\"\n",
        "\n",
        "def transform_documents(input_documents):\n",
        "    original_documents = treebank.sents(input_documents)\n",
        "    lowercased_documents = [list(map(str.lower, sentence)) for sentence in original_documents]\n",
        "    # Replace digits in each sentence of original_documents\n",
        "    # Join the words into a string, replace digits, then split back into words\n",
        "    abstractdigit_documents = [\n",
        "        re.sub(r'\\d', digit_replacement_symbol, ' '.join(sentence)).split()\n",
        "        for sentence in original_documents\n",
        "    ]\n",
        "    return abstractdigit_documents, lowercased_documents, original_documents\n",
        "\n",
        "abstractdigit_train, lowercased_train, original_train = transform_documents(train_documents)\n",
        "abstractdigit_test, lowercased_test, original_test = transform_documents(test_documents)"
      ],
      "metadata": {
        "id": "0VLalLZgbYCD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate vocabulary\n",
        "\n",
        "One vocabulary set will be created for each type of dataset (original,lowercased, abstract digit). The generate_vocabulary function forms a set of unique words that are existent in the train corpus with minimum frequency of value 3. Counter is used to measure the frequency of each token in the dataset. The pairs of token and count are returned by the method. Variables bigr and trigr represent the order of the n-grams. ka and kb are smoothing parameters of add-k smoothing technique. num_sent param shows the number of sentences that will be generated by the n-gram models."
      ],
      "metadata": {
        "id": "lAdMKME4kxv6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_vocabulary(frequency_threshold, documents):\n",
        "    word_counter = Counter()\n",
        "    for sentence in documents:\n",
        "        word_counter.update(sentence)\n",
        "    return set(word for word, frequency in word_counter.items() if frequency >= frequency_threshold)\n",
        "\n",
        "bigr, trigr,  min_frequency, ka, kb, num_sent = 2, 3, 3, 1, 0.01, 3\n",
        "\n",
        "vocabulary_original = generate_vocabulary(min_frequency, original_train)\n",
        "vocabulary_lowercased = generate_vocabulary(min_frequency, lowercased_train)\n",
        "vocabulary_abstractdigit = generate_vocabulary(min_frequency, abstractdigit_train)"
      ],
      "metadata": {
        "id": "uTD9p84ZbcqN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess text\n",
        "\n",
        "This function pads every sentence of the corpus with start and end symbols, which are ```<BOS>``` and ```<EOS>``` respectively. Then it creates n-grams of order n specified by the argument \"n\" of the function. The n-grams are filtered to remove any of them that have more than one occurence of start or end symbol. If a word is not in the specified vocabulary, the function replaces that with a token called ```<UNK>```.\n",
        "\n"
      ],
      "metadata": {
        "id": "RP8uoThvHnxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(start_symbol, end_symbol,oov_label, documents, vocabulary, n):\n",
        "    # Initialize the list for storing the processed n-grams\n",
        "    ngrams_processed = []\n",
        "\n",
        "    for sentence in documents:\n",
        "        # Add start and end symbols to each sentence before generating n-grams\n",
        "        padded_sentence = [start_symbol] + sentence + [end_symbol]\n",
        "\n",
        "        # Generate n-grams for the padded sentence\n",
        "        sentence_ngrams = list(ngrams(padded_sentence, n))\n",
        "\n",
        "        # Directly filter and append the n-grams that meet the criteria to the result list\n",
        "        ngrams_processed.extend([ngram for ngram in sentence_ngrams if not (\n",
        "            ngram[:n-1].count(end_symbol) > 0 or ngram[1:].count(start_symbol) > 0)])\n",
        "\n",
        "    ngrams_replaced = []  # Initialize an empty list to hold the processed n-grams\n",
        "\n",
        "    # Loop through each n-gram in the list of processed n-grams\n",
        "    for ngram in ngrams_processed:\n",
        "        replaced_ngram = []  # Initialize a list to hold the tokens of the current n-gram after processing\n",
        "\n",
        "        # Loop through each token in the current n-gram\n",
        "        for token in ngram:\n",
        "            # Check if the token is in the vocabulary or is a start/end symbol\n",
        "            if token in vocabulary or token in {start_symbol, end_symbol}:\n",
        "                replaced_ngram.append(token)  # Keep the token as it is\n",
        "            else:\n",
        "                replaced_ngram.append(oov_label)  # Replace the token with the out-of-vocabulary label\n",
        "\n",
        "        # Add the processed n-gram (as a tuple) to the list of replaced n-grams\n",
        "        ngrams_replaced.append(tuple(replaced_ngram))\n",
        "\n",
        "\n",
        "    return ngrams_replaced"
      ],
      "metadata": {
        "id": "ljQ0Q9JsiC1D"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and test n-gram model\n",
        "\n",
        "The first function computes the perplexity of the n-gram model. For this purpose, probabilities of the n-gram's as well as base (prefix) probabilities are utilized.  The language model's probabilities are determined by the likelihood of each suffix given its prefix through k-smoothing. To compute them, we use counters of prefix(base) and suffix(follow) produced by the n-grams. A dictionary of counters is returned.\n",
        "\n",
        "To measure how the model performs we calculate the perplexity of the model on the given n-grams. We need to have perplexity as low as possible to have better performance. N-grams from test set are used to calcualte perplexity."
      ],
      "metadata": {
        "id": "rd0vsEzdKNQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "def compute_perplexity(tested_ngrams, vocabulary_set, model, base_probs):\n",
        "    log_probability_total = sum(\n",
        "        math.log(model.get(gram[:-1], {}).get(gram[-1], base_probs.get(gram[:-1], 1 / len(vocabulary_set))))\n",
        "        for gram in tested_ngrams\n",
        "    )\n",
        "    return math.exp(-log_probability_total / len(tested_ngrams))\n",
        "\n",
        "def initialize_model(sequence_of_ngrams, smoothing_factor, vocabulary_set):\n",
        "    language_model = defaultdict(lambda: defaultdict(float))\n",
        "    total_counts = Counter(sequence_of_ngrams)\n",
        "    base_probabilities = {}\n",
        "\n",
        "    # Calculate base counts and probabilities\n",
        "    base_counts = defaultdict(int)\n",
        "    for seq, freq in total_counts.items():\n",
        "        base = seq[:-1]\n",
        "        base_counts[base] += freq\n",
        "        language_model[base][seq[-1]] = freq  # Temporarily store raw frequency\n",
        "\n",
        "    # Adjust counts for smoothing and calculate base probabilities\n",
        "    vocab_length = len(vocabulary_set)\n",
        "    for base, total in base_counts.items():\n",
        "        base_probabilities[base] = smoothing_factor / (total + smoothing_factor * vocab_length)\n",
        "        for follow in language_model[base]:\n",
        "            language_model[base][follow] = (language_model[base][follow] + smoothing_factor) / (total + smoothing_factor * vocab_length)\n",
        "\n",
        "    return language_model, base_probabilities\n",
        "\n",
        "def process_text(train_sequences, vocabulary_set, test_sequences, method_flag, n_value, smoothing_factor):\n",
        "    trained_model, probabilities = initialize_model(train_sequences, smoothing_factor, vocabulary_set)\n",
        "    perplexity_value = compute_perplexity( test_sequences, vocabulary_set, trained_model, probabilities)\n",
        "\n",
        "    print(f\"{'Bigram' if n_value == 2 else 'Trigram'} model | Method = {method_flag} | Smoothing k = {smoothing_factor} | Perplexity = {perplexity_value}\")\n",
        "\n",
        "    return trained_model\n"
      ],
      "metadata": {
        "id": "p91Nq6BStOTc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate sentences\n",
        "\n",
        "The function searches for a n-gram that begins with the init symbol randomly. Since n-grams are used to form a sentence, we need prefixes to produce a new word. The valid following words in a sentence are chosen based on the weighted probability produced by the n-gram model. At each iteration, the program checks if the stop symbol is reached to end the sentence. If the algorithm does not find any valid next words, it stops generating a sentence and tries to create a new one until the sentence's threshold is reached."
      ],
      "metadata": {
        "id": "tyK2uZDYYYsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sentences(init, stop, model, order, allowed_words, sentences_count):\n",
        "    for index in range(sentences_count):\n",
        "        # Starting with a randomly chosen n-gram that begins with the initial token.\n",
        "        current_ngram = random.choice([key for key in model if key[0] == init])\n",
        "        text = list(current_ngram[:order-1])\n",
        "\n",
        "        # Build the sentence.\n",
        "        while text[-1] != stop:\n",
        "            current_prefix = tuple(text[-order+1:])\n",
        "            # Stop if the current prefix is not in the model or cannot proceed further.\n",
        "            if current_prefix not in model or not model[current_prefix]:\n",
        "                text += [stop]\n",
        "                break\n",
        "\n",
        "            # Filter candidates based on vocabulary and end token, then choose the next word.\n",
        "            valid_next_words = []\n",
        "            for word, prob in model.get(current_prefix, {}).items():\n",
        "                if word in allowed_words or word == stop:\n",
        "                    valid_next_words.append((word, prob))\n",
        "\n",
        "            if not valid_next_words:\n",
        "                text += [stop]\n",
        "                break\n",
        "\n",
        "            # Section for weighted selection:\n",
        "            new_word, probability = zip(*valid_next_words)\n",
        "            next_word = random.choices(new_word, weights=probability)[0]\n",
        "            text.append(next_word)\n",
        "\n",
        "        # Output the generated sentence.\n",
        "        formatted_sentence = \" \".join(text)\n",
        "        print(f\"Sentence {index + 1}: {formatted_sentence}\")"
      ],
      "metadata": {
        "id": "IkqmR6I-qWH2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train0 = preprocess_text(\"<BOS>\", \"<EOS>\", \"<UNK>\",original_train, vocabulary_original, bigr)\n",
        "test0 = preprocess_text(\"<BOS>\", \"<EOS>\", \"<UNK>\", original_test, vocabulary_original, bigr)\n",
        "bigram_model_original_ka = process_text(train0 , vocabulary_original, test0 , \"original\", bigr, ka)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUi31mp4fqjb",
        "outputId": "840b08f4-5611-4746-e058-bccc61501791"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram model | Method = original | Smoothing k = 1 | Perplexity = 383.50361532871557\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating sentences with bigram model trained on original text, smoothing factor=1\n",
        "\n",
        "The sentences generated by this model have some problems with grammar and syntax. The context of the texts is not very meaningful but there is some level of logical structure and coherence. It uses punctuation but not very accurately.  \n",
        "\n",
        "The first sentence is very short and lacks meaningful context that could be provided with some verb. Also, the two apostrophes at the start of the first sentence were not needed at all. The sentence ends with full-stop which is correct punctuation.\n",
        "\n",
        "Regarding the second sentence, it lacks meaning too. Some nations' right plan is reffered and some character named Moleculon. It is not coherent since there are a lot of uncorrelated parts in the sentence. It is not grammticaly correct at all.\n",
        "\n",
        "The third sentence has slightly better meaning than the previous two. The syntax and grammar that was used is not accurate. The sentence suggests some warning signals based on evidence provided by test scores. Also, the part\"Michael P. Sullivan , or had reached a regulatory life on what the House and consented to own tax rates of the paper on cost-sharing\" could raise concern about tax-rates and cost sharing. Finally, ```<UNK>``` token is not included in the text.  \n"
      ],
      "metadata": {
        "id": "uljcnuX28tn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sentences(\"<BOS>\", \"<EOS>\",bigram_model_original_ka, bigr, vocabulary_original, num_sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqNe8YuXiRil",
        "outputId": "08191d83-8c8e-430d-88d5-df27b84b8b0e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: <BOS> `` really only modestly . <EOS>\n",
            "Sentence 2: <BOS> These nations over rights plan if its Moleculon 's in the company . <EOS>\n",
            "Sentence 3: <BOS> Savin Corp. , provided evidence of test scores are warning signals any time 0 prices , you are generally expected premium over # 8.5 % from developing nations are prepared statement 0 conditions were low for the issue trades of exports increased number ; Michael P. Sullivan , or had reached a regulatory life on what the House and consented to own tax rates of the paper on cost-sharing . <EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train1 = preprocess_text(\"<BOS>\", \"<EOS>\", \"<UNK>\", lowercased_train, vocabulary_lowercased, bigr)\n",
        "test1 = preprocess_text(\"<BOS>\", \"<EOS>\", \"<UNK>\", lowercased_test, vocabulary_lowercased, bigr)\n",
        "bigram_model_lowercased_ka = process_text(train1, vocabulary_lowercased, test1, \"lowercase\", bigr, ka)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cuc2R2t0jOWA",
        "outputId": "d4653402-b9a6-427f-8206-91b41835b332"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram model | Method = lowercase | Smoothing k = 1 | Perplexity = 383.9460197558427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sentences(\"<BOS>\", \"<EOS>\",bigram_model_lowercased_ka, bigr, vocabulary_lowercased, num_sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DaJhblYjVlz",
        "outputId": "8aa6af2a-a4d8-4410-e630-76ad427ab9ec"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: <BOS> she says *t*-1 . <EOS>\n",
            "Sentence 2: <BOS> that *t*-1 . <EOS>\n",
            "Sentence 3: <BOS> as profit *ich*-1 said 0 they add it in a magazine , st. louis sullivan , he owns and connecticut will continue *-3 to 8 1\\/2 % rate on 30-year bond prices closed up 1 order *-1 talking , editor and gyrations of high-tech medical association through another only offers a slowing in october at $ 500 stocks in fiscal 1990 , from # 6 5\\/8 , '' <EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train2 = preprocess_text(\"<BOS>\", \"<EOS>\", \"<UNK>\",original_train, vocabulary_original, bigr)\n",
        "test2 = preprocess_text(\"<BOS>\", \"<EOS>\", \"<UNK>\",original_test, vocabulary_original, bigr)\n",
        "bigram_model_original_kb = process_text(train2 , vocabulary_original, test2,\"original\", bigr, kb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lopjgu86jZdx",
        "outputId": "af2202ef-274b-48cd-9da7-aec582b01dea"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram model | Method = original | Smoothing k = 0.01 | Perplexity = 137.81108464477174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating sentences with bigram model trained on original text, smoothing factor=0.01\n",
        "\n",
        "The coherence is improved on this experiment, since there is better logical structure of the words. The content has better meaning in this situation than when smoothing factor was 1.\n",
        "\n",
        "In the first sentence there are two subsentences, separated by comma. The first part tells us that the screen shows European economic slowdown in the top 10%. The second part suggest to reform law enforcement and the economics. The content here makes more sense than before but still it is not very clear.  \n",
        "\n",
        "The second sentence is not very accurate and lacks meaningful context. Someone can understand that troubled Media Heritage proposed something about California Health organization. Also, we can swap dealing is a grammatically correct phrase\n",
        "with proper syntax but it does not fit in this sentence.\n",
        "\n",
        "The third sentence has no coherence and the numbers provided are very confusing.\n",
        "There are probably some claims about thoughts regarding Goldman Sachs but no reasonable context is provided unfortunately. \"It should set\" and \"i know\" show that correct syntax was used partially but the sentence as a whole does not convey any meaningful message."
      ],
      "metadata": {
        "id": "B386EdbIFc1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sentences(\"<BOS>\", \"<EOS>\", bigram_model_original_kb, bigr, vocabulary_original, num_sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhAlGPUyBav7",
        "outputId": "3028a0b2-39e2-4531-814f-ba3e31329bbb"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: <BOS> But the screen shows two European economic slowdown in the top 10 % , '' said 0 insurance reform law enforcement and its economics . <EOS>\n",
            "Sentence 2: <BOS> After troubled Heritage Media proposed * with a California Health Organization and we can swap dealings . <EOS>\n",
            "Sentence 3: <BOS> This is thought 0 it should set *-1 $ 15,000 *U* and I know 0 *T*-1 Goldman Sachs . <EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train3 = preprocess_text(\"<BOS>\",\"<EOS>\", \"<UNK>\",lowercased_train, vocabulary_lowercased, bigr)\n",
        "test3 = preprocess_text(\"<BOS>\", \"<EOS>\", \"<UNK>\", lowercased_test, vocabulary_lowercased, bigr)\n",
        "bigram_model_lowercased_kb = process_text(train3 , vocabulary_lowercased, test3 ,\"lowercase\", bigr, kb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvylFOSvjciT",
        "outputId": "9834182d-5ec1-4614-bd2d-527388815652"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram model | Method = lowercase | Smoothing k = 0.01 | Perplexity = 143.78868465313255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sentences(\"<BOS>\", \"<EOS>\", bigram_model_lowercased_kb, bigr, vocabulary_lowercased, num_sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfnQyUl3jgU4",
        "outputId": "0eabc5a8-1162-4fd5-bc36-172da59b9ab0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: <BOS> the concept 's a compensation program trading halt proposal comes as a woman in the previous contract modestly as the former colleagues that drug administration officials began in the first three with other requirements that post a woman in an editorial product was full membership in other with fewer than a package designed * to consider them . <EOS>\n",
            "Sentence 2: <BOS> `` side of the usx case . <EOS>\n",
            "Sentence 3: <BOS> attorneys have wa '' mr. baldwin , including net income for common shares . <EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train4 = preprocess_text(\"<BOS>\", \"<EOS>\", \"<UNK>\", original_train, vocabulary_original, trigr)\n",
        "test4 = preprocess_text(\"<BOS>\", \"<EOS>\", \"<UNK>\",  original_test, vocabulary_original, trigr)\n",
        "trigram_model_original_ka = process_text(train4, vocabulary_original, test4, \"original\", trigr, ka)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-U4uUZeKjiGx",
        "outputId": "6d1729e9-b561-4123-e1df-26bc9ed9b112"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trigram model | Method = original | Smoothing k = 1 | Perplexity = 1504.6100128907715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating sentences with trigram model trained on original text, smoothing factor=1\n",
        "\n",
        "The coherence and logical structure is further improved on this experiment. The content has better meaning in this situation than when bigrams were used.\n",
        "\n",
        "The first sentence conveys a very clear message and the whole sentence has a proper meaning. It claims that Taiwan has improved its position thanks to some company's work on clean air legislation. The syntax is totally correct as well as grammar and punctuation. Overall, the result is very satisfying since it conveys a logical message.\n",
        "\n",
        "Unfortunately, the second sentece contains only one word and not much information can be extracted from it. Nevertheless, starting and ending token are used correctly and the ```<UNK>``` token is avoided here too.\n",
        "\n",
        "The third sentence is problematic. Although, punctuation is used accurately, the sentence lacks proper meaning. It does not convey any logical message but the syntax is correct. It could suggest something about earnings regarding some research but no concrete meaning can be extracted. Grammar is wrong since plural form should be used(aka spinoffs instead of spinoff)."
      ],
      "metadata": {
        "id": "GaYTLZjGJw0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sentences(\"<BOS>\", \"<EOS>\",trigram_model_original_ka, trigr,  vocabulary_original, num_sent)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzxVBia1jjzH",
        "outputId": "2c2eabcc-5c18-4858-b372-b439745ecf1e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: <BOS> Taiwan has improved its standing with the company 's work on clean-air legislation . <EOS>\n",
            "Sentence 2: <BOS> FEDERAL <EOS>\n",
            "Sentence 3: <BOS> However , none of the spinoff caused Cray Research 's earnings . <EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train5 = preprocess_text(\"<BOS>\", \"<EOS>\", \"<UNK>\", lowercased_train, vocabulary_lowercased, trigr)\n",
        "test5 = preprocess_text(\"<BOS>\", \"<EOS>\", \"<UNK>\", lowercased_test, vocabulary_lowercased, trigr)\n",
        "trigram_model_lowercased_ka = process_text(train5,  vocabulary_lowercased, test5, \"lowercase\", trigr, ka)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ftsrz_xCjlcJ",
        "outputId": "1e235fa9-17b0-4d16-d3b0-2400d3514efe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trigram model | Method = lowercase | Smoothing k = 1 | Perplexity = 1470.5319718904307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sentences(\"<BOS>\", \"<EOS>\", trigram_model_lowercased_ka , trigr, vocabulary_lowercased, num_sent)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHcDct8OjnyG",
        "outputId": "8dbff15b-d016-4f15-fb3a-37e2c064213a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: <BOS> thus , an announcer talks about the new , lower in seoul , will be eligible for duty-free treatment . <EOS>\n",
            "Sentence 2: <BOS> dividend growth next year based on actual <EOS>\n",
            "Sentence 3: <BOS> 20 billion *u* of japanese investors as a result , ms. ganes . <EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train6 = preprocess_text(\"<BOS>\", \"<EOS>\", \"<UNK>\", original_train, vocabulary_original, trigr)\n",
        "test6 = preprocess_text(\"<BOS>\", \"<EOS>\", \"<UNK>\", original_test, vocabulary_original, trigr)\n",
        "trigram_model_orginal_kb  = process_text(train6, vocabulary_original, test6, \"original\", trigr, kb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zhvFEhHj2yw",
        "outputId": "8a39504d-0553-4b0b-c448-002ddcd9c6d9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trigram model | Method = original | Smoothing k = 0.01 | Perplexity = 463.80467915524156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating sentences with trigram model trained on original text, smoothing factor=0.01\n",
        "\n",
        "From the experiment, it is understood that trigram models produce shorter sentences compared to bigram models. There is some coherence in this model too.\n",
        "The first sentece has some logical structure, but the other two are confusing.\n",
        "\n",
        "The first sentence is very logical and conveys a clear message. It states that Mary Beth's mother did not see any signs that inventories are excessive. Grammar, syntax and punctuation are entirely correct. The two apostrophes at the end of the first sentence were not needed though. Nevertheless, excessive inventories  is any inventory that exceeds projected demand, and is therefore not expected to sell. Based on the structure of the given dataset, this sentence makes a lot of sense.\n",
        "\n",
        "However, the second sentece is very short and not many conclusions can be extracted from it.\n",
        "\n",
        "Unfortuantely, the third second is not correct. It lacks logical structure and syntax, showcasing that the model can be inefficient. The first part of the sentence suggests that there are some signs about some growth but it does not state clearly why this could happen. Also, it is stated that an institution might need help but the meaning is very vague. Grammar is not significantly incorrect while syntax definitely is. There are some characters that were not needed in the sentence, such as '*'."
      ],
      "metadata": {
        "id": "y0ohWcesOLOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sentences(\"<BOS>\", \"<EOS>\", trigram_model_orginal_kb , trigr, vocabulary_original, num_sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwGJwC-Aj5xF",
        "outputId": "d398e134-b9c4-4d5a-fa69-9972553efe3f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: <BOS> Mary Beth 's mother did n't see any signs that inventories are excessive . '' <EOS>\n",
            "Sentence 2: <BOS> -LRB- During its <EOS>\n",
            "Sentence 3: <BOS> While there were signs that growth is coming to a round of bilateral economic talks scheduled * for delivery last night to a savings institution needs your help now ! <EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train7 = preprocess_text(\"<BOS>\", \"<EOS>\", \"<UNK>\", lowercased_train, vocabulary_lowercased, trigr)\n",
        "test7 = preprocess_text(\"<BOS>\", \"<EOS>\", \"<UNK>\", lowercased_test, vocabulary_lowercased, trigr)\n",
        "trigram_model_lowercased_kb = process_text(train7 ,vocabulary_lowercased, test7 , \"lowercase\", trigr, kb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbsRfvZujpWF",
        "outputId": "590152e4-26c1-4c5d-af72-659274706127"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trigram model | Method = lowercase | Smoothing k = 0.01 | Perplexity = 461.769817591675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sentences(\"<BOS>\", \"<EOS>\", trigram_model_lowercased_kb, trigr, vocabulary_lowercased, num_sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nS_bLJYjrNJ",
        "outputId": "69c8addd-807e-453f-e350-704184f1329d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: <BOS> douglas madison , a problem created * by president bush 's job performance and 85 % of its new customers each week , u.s. trade representative carla hills to use their pill must still pay their share of a rival . <EOS>\n",
            "Sentence 2: <BOS> he has a yield of about $ 1.1 billion *u* in food aid had <EOS>\n",
            "Sentence 3: <BOS> associates say 0 *t*-1 was put $ 15,000 *u* fine ; john p. <EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train8 = preprocess_text(\"<BOS>\", \"<EOS>\", \"<UNK>\", abstractdigit_train, vocabulary_abstractdigit, bigr)\n",
        "test8 = preprocess_text(\"<BOS>\", \"<EOS>\", \"<UNK>\", abstractdigit_test, vocabulary_abstractdigit, bigr)\n",
        "bigram_model_abstarctdigit_ka = process_text(train8 , vocabulary_abstractdigit, test8 , \"abstract digit\", bigr, ka)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXIz72QPyZNw",
        "outputId": "370e8168-5b9c-45de-f797-d4a8f4232d36"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram model | Method = abstract digit | Smoothing k = 1 | Perplexity = 351.3990885618452\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sentences(\"<BOS>\", \"<EOS>\", bigram_model_abstarctdigit_ka, bigr, vocabulary_abstractdigit, num_sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnL1ebh7zysw",
        "outputId": "2a4ccd0e-b9c7-4cc7-9b80-0d86044eb89d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: <BOS> The bids for the U.S. Embassy . <EOS>\n",
            "Sentence 2: <BOS> Douglas Madison , St. Mary Beth 's probably will direct an interview . <EOS>\n",
            "Sentence 3: <BOS> One , with analysts say # Chrysler Corp. , Philadelphia Fed officials could use the union 's school was for results *T*-# to send your own intended *-# to set *-# *-# to increase in the very least costs throughout the IRS in computers and testing . <EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train9 = preprocess_text(\"<BOS>\", \"<EOS>\", \"<UNK>\", abstractdigit_train, vocabulary_abstractdigit, bigr)\n",
        "test9 = preprocess_text(\"<BOS>\", \"<EOS>\", \"<UNK>\", abstractdigit_test, vocabulary_abstractdigit, bigr)\n",
        "bigram_model_abstarctdigit_kb = process_text(train9 , vocabulary_abstractdigit, test9 , \"abstract digit\", bigr, kb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52yKR1F23bum",
        "outputId": "021df8d2-fc22-430b-9c5a-be01762734f3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigram model | Method = abstract digit | Smoothing k = 0.01 | Perplexity = 125.38175697463285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sentences(\"<BOS>\", \"<EOS>\", bigram_model_abstarctdigit_kb, bigr, vocabulary_abstractdigit, num_sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SuenK-K3vQg",
        "outputId": "f4006057-2531-498f-bc62-deec07b64e9e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: <BOS> The cases , however , teacher just an analyst who *T*-## came together not be available *ICH*-# on alcohol and Scott Paper gained ##.## billion yen compared with their daily contracts traded portfolios are common form of UAL stock index , Calif. , five bells is n't *?* the situation , Republican Rudolph Giuliani 's meeting in October #### second incentive plan the fourth among investors have had been hit the many warrants , who *T*-### are no more customers by major markets themselves with cheaper electrical current-carrying capacity ### million to join a Chemical Banking Corp. , deal is that firms ' `` Insurance Institute . <EOS>\n",
            "Sentence 2: <BOS> In an appeal , referred the appropriations bill is what *T*-## took over policy , or anyone has `` really need the rates , both Massachusetts banks . '' <EOS>\n",
            "Sentence 3: <BOS> Mrs. Yeargin 's short-term investor is just as well below #### , if you want *-# to cut the Bank . <EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train10 = preprocess_text(\"<BOS>\", \"<EOS>\", \"<UNK>\", abstractdigit_train, vocabulary_abstractdigit, trigr)\n",
        "test10 = preprocess_text(\"<BOS>\", \"<EOS>\", \"<UNK>\", abstractdigit_test, vocabulary_abstractdigit, trigr)\n",
        "trigram_model_abstarctdigit_ka = process_text(train10 , vocabulary_abstractdigit, test10 , \"abstract digit\", trigr, ka)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFiK9t_8326i",
        "outputId": "fa1541f9-74be-40de-8c84-49d4d3532554"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trigram model | Method = abstract digit | Smoothing k = 1 | Perplexity = 1389.017984976645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sentences(\"<BOS>\", \"<EOS>\", trigram_model_abstarctdigit_ka, trigr, vocabulary_abstractdigit, num_sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOSgp4T74Ge0",
        "outputId": "09829eec-7e79-45e5-903c-905ae3e75624"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: <BOS> Judge Curry added an additional $ ## million *U* a share , on the way for # ### million *U* a bottle . <EOS>\n",
            "Sentence 2: <BOS> Sir Peter will succeed Mr. McAlpine could be lowered *-# soon , immediately bid up shares of common stock of record Nov. ## , vice president with Morgan Stanley there , '' so `` by request , '' he said # its trucks and minivans to be signed *-## by the Giuliani ads , Mr. Yamamoto said , `` your TV ad needs *-# to appear in the futures market . <EOS>\n",
            "Sentence 3: <BOS> Macmillan\\/McGraw says # the significant drop in interest rates and the medical schools of <EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train11 = preprocess_text(\"<BOS>\", \"<EOS>\", \"<UNK>\", abstractdigit_train, vocabulary_abstractdigit, trigr)\n",
        "test11 = preprocess_text(\"<BOS>\", \"<EOS>\", \"<UNK>\", abstractdigit_test, vocabulary_abstractdigit, trigr)\n",
        "trigram_model_abstarctdigit_kb = process_text(train11 , vocabulary_abstractdigit, test11 , \"abstract digit\", trigr, kb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eb0xrILC4Q7r",
        "outputId": "288a3214-e01c-44dc-e504-164b2bfaf42a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trigram model | Method = abstract digit | Smoothing k = 0.01 | Perplexity = 412.4322411400487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sentences(\"<BOS>\", \"<EOS>\", trigram_model_abstarctdigit_kb, trigr, vocabulary_abstractdigit, num_sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMpsQpOV4ai1",
        "outputId": "a954f51a-09f5-4395-b2cf-7a2055cd2426"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1: <BOS> Taiwan has improved its standing with the language '' in Asia to $ #.# billion *U* of high-yield , <EOS>\n",
            "Sentence 2: <BOS> Charles D. <EOS>\n",
            "Sentence 3: <BOS> USX said # they may very well not <EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Languange Model | Original Text | Lowercased Text | Abstract Digits Text |\n",
        "|----|----|----|----|\n",
        "| Bigrams(k=1) | 383.50361532871557 | 383.9460197558427 | 351.3990885618452 |\n",
        "| Bigrams(k=0.01) | 137.81108464477174 | 143.78868465313255|  125.38175697463285 |\n",
        "| Trigrams(k=1) | 1504.6100128907715 | 1470.5319718904307 | 1389.017984976645 |\n",
        "| Trigrams(k=0.01) | 463.80467915524156 | 461.769817591675 | 412.4322411400487 |\n",
        "\n",
        "From the matrix above, it is observed that the best smoothing factor for both bigrams and trigrams is 0.01. Bigrams always have lower perplexity than trigrams in this experiment, which means that the model performs better on them. Lowercase text increases slightly the perplexity for bigrams. On the contrary, it reduces the perplexity scores for trigrams compared to original text. So, lowercase text is more suitable for trigrams than bigrams. Abstract digit transformation improves the scores of the original text in all of the experiments, regardless smoothing factor k. Abstract digit's perplexity scores are consistently the lowest in the experiment which means that the model has the best performance on this type of data.\n",
        "Regarding the range of the values, those of 120-150 indicate that the model can predict the sentences of the text very accurately. When the values range from 380 to 460, the effectiveness of the model drops a little but it is still satisfying at predicting the corpus. Perplexity of 1000 to 1500 shows that the model's accuracy is poor and cannot predict the text properly.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yt8xM4cuwKd2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat Gpt-4 Prompt\n",
        "\n",
        "I commented out code that was not working, and added some lines in order to make it functional. Someone can read the code to see the editing that was made\n",
        "\n",
        "---------------------------------\n",
        "\n",
        "Prompt:\n",
        "\n",
        "I'm working on a natural language processing project and need to preprocess my text data. Could you provide a Python function that takes in several parameters including a start symbol, an end symbol, an out-of-vocabulary label, a list of sentences (documents), a vocabulary set, and an integer n for generating n-grams? The function should add start and end symbols to each sentence, generate n-grams, and filter out any n-grams that have the start symbol not at the beginning or the end symbol not at the end. Additionally, it should replace any tokens not found in the provided vocabulary with the out-of-vocabulary label. Finally, the function should return the list of processed n-grams. It's important that the start and end symbols are treated as valid tokens and not replaced\n",
        "\n",
        "\n",
        "### Follow this algorithm:\n",
        "###  Initialize the list for storing the processed n-grams\n",
        "###  Add start and end symbols to each sentence before generating n-grams\n",
        "###         Generate n-grams for the padded sentence\n",
        "###         Directly filter and append the n-grams that meet the criteria to the result list\n",
        "### Initialize an empty list to hold the processed n-grams\n",
        "###     Loop through each n-gram in the list of processed n-grams\n",
        "    \n",
        "###         Initialize a list to hold the tokens of the current n-gram after processing\n",
        "\n",
        "###         Loop through each token in the current n-gram\n",
        "###           Keep the token as it is\n",
        "     \n",
        "###                  Replace the token with the out-of-vocabulary label\n",
        "###        Add the processed n-gram (as a tuple) to the list of replaced n-grams"
      ],
      "metadata": {
        "id": "fdSpQZ5g_JTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removed these lines (with star)\n",
        "# *from typing import List, Set, Tuple\n",
        "\n",
        "# *def generate_ngrams(sentences: List[str], vocab: Set[str], start_symbol: str, end_symbol: str, out_of_vocab: str, n: int) -> List[Tuple[str, ...]]:\n",
        "# *    def ngrams(input_list, n):\n",
        "# *        return [tuple(input_list[i:i+n]) for i in range(len(input_list)-n+1)]\n",
        "def preprocess_text2(start_symbol, end_symbol,out_of_vocab, sentences, vocab, n):\n",
        "    processed_ngrams = []\n",
        "    # Adding start and end symbols, and generating n-grams\n",
        "    for sentence in sentences:\n",
        "\n",
        "       # Removed .split() from sentence, it was not needed\n",
        "        padded_sentence = [start_symbol] * (n-1) + sentence + [end_symbol] * (n-1)\n",
        "        sentence_ngrams = ngrams(padded_sentence, n)\n",
        "\n",
        "        # added this line\n",
        "        # * Directly filter and append the n-grams that meet the criteria to the result list\n",
        "        processed_ngrams.extend([ngram for ngram in sentence_ngrams if not (\n",
        "            ngram[:n-1].count(end_symbol) > 0 or ngram[1:].count(start_symbol) > 0)])\n",
        "\n",
        "        # Removed these lines\n",
        "        # # *Filtering out n-grams with misplaced start and end symbols\n",
        "        # *for ngram in sentence_ngrams:\n",
        "        #  *   if ngram[0] == start_symbol and ngram[-1] == end_symbol:\n",
        "        #   *      processed_ngrams.append(ngram)\n",
        "\n",
        "    # Replace tokens not in vocabulary\n",
        "    replaced_ngrams = []\n",
        "    for ngram in processed_ngrams:\n",
        "        replaced_ngram = []\n",
        "        for token in ngram:\n",
        "            if token in vocab or token in [start_symbol, end_symbol]:\n",
        "                replaced_ngram.append(token)\n",
        "            else:\n",
        "                replaced_ngram.append(out_of_vocab)\n",
        "        replaced_ngrams.append(tuple(replaced_ngram))\n",
        "\n",
        "    return replaced_ngrams\n"
      ],
      "metadata": {
        "id": "GJ7gTRcq7HVl"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat GPT-4 prompts\n",
        "\n",
        "First prompt:\n",
        "I need a Python script that accomplishes the following tasks for working with n-gram language models, specifically designed to calculate the perplexity of a language model on a set of test data:\n",
        "\n",
        "Compute Perplexity: Given a list of tested n-grams, a set representing the vocabulary, a pre-trained language model, and a dictionary of base probabilities for unseen n-grams, calculate the perplexity of the model on the test data. Perplexity should be computed as the exponent of the negative average log probability across all tested n-grams.\n",
        "\n",
        "Initialize Model: Initialize a language model from a sequence of n-grams. This involves:\n",
        "\n",
        "Counting the occurrences of each n-gram in the training data.\n",
        "Applying a smoothing factor to adjust for n-grams that do not appear in the training data, ensuring that the model can handle unseen n-grams.\n",
        "Calculating the smoothed probabilities for each n-gram and the base probability for unseen n-grams within known contexts.\n",
        "The model should be represented as a nested dictionary where the first level corresponds to the (n-1)-gram contexts, and the second level maps the nth word of the n-gram to its probability given the context.\n",
        "\n",
        "Process Text: This function should tie everything together. It takes as input:\n",
        "\n",
        "The sequences of n-grams from the training data.\n",
        "A vocabulary set derived from the training data.\n",
        "The sequences of n-grams from the test data.\n",
        "A method flag indicating the type of smoothing applied ('Laplace' for example).\n",
        "An n_value indicating whether the model is a bigram or trigram model.\n",
        "A smoothing factor to be applied during model initialization.\n",
        "The function should train the model with the training data and then compute and print the perplexity of the test data.\n",
        "\n",
        "Please include appropriate comments in the code to explain the purpose and functionality of each part\n",
        "\n",
        "\n",
        "------------------------------------\n",
        "Second prompt:\n",
        "\n",
        "Input is of this form \"\n",
        "```[ ('<BOS>', '<UNK>', '<UNK>'), ('<UNK>', '<UNK>', ','),('<UNK>', ',', '61')```"
      ],
      "metadata": {
        "id": "PcYn9SIHqTo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "def train_model2(training_ngrams, vocabulary, n_value, smoothing='Laplace', smoothing_factor=1):\n",
        "    \"\"\"\n",
        "    Initialize the language model from the sequence of n-grams.\n",
        "\n",
        "    :param training_ngrams: A list of n-grams from the training data.\n",
        "    :param vocabulary: A set representing the unique words in the training data, including special tokens.\n",
        "    :param n_value: The n in n-gram (e.g., 2 for bigrams, 3 for trigrams).\n",
        "    :param smoothing: The type of smoothing applied ('Laplace' for now).\n",
        "    :param smoothing_factor: The smoothing factor to adjust for unseen n-grams.\n",
        "    :return: A nested dictionary representing the n-gram model and base probabilities.\n",
        "    \"\"\"\n",
        "    model = defaultdict(lambda: defaultdict(float))\n",
        "    context_counts = defaultdict(int)\n",
        "\n",
        "    # Count occurrences of each n-gram and its context\n",
        "    for ngram in training_ngrams:\n",
        "        context = ngram[:-1]\n",
        "        word = ngram[-1]\n",
        "        model[context][word] += 1\n",
        "        context_counts[context] += 1\n",
        "\n",
        "    # Calculate probabilities with smoothing for each context\n",
        "    for context, words in model.items():\n",
        "        total_count = context_counts[context]\n",
        "        denominator = total_count + (smoothing_factor * len(vocabulary))\n",
        "        for word in words:\n",
        "            words[word] = (words[word] + smoothing_factor) / denominator\n",
        "\n",
        "    # Calculate base probability for unseen n-grams in any context\n",
        "    base_probability = smoothing_factor / (len(vocabulary) + smoothing_factor * len(vocabulary))\n",
        "\n",
        "    return model, base_probability\n",
        "\n",
        "def compute_perplexity2(test_ngrams, model, base_probability):\n",
        "    \"\"\"\n",
        "    Compute the perplexity of the model on the test data.\n",
        "\n",
        "    :param test_ngrams: A list of tested n-grams, including special tokens.\n",
        "    :param model: The trained n-gram model.\n",
        "    :param base_probability: Base probability for unseen n-grams.\n",
        "    :return: The perplexity score.\n",
        "    \"\"\"\n",
        "    log_probability_sum = 0\n",
        "    for ngram in test_ngrams:\n",
        "        context = ngram[:-1]\n",
        "        word = ngram[-1]\n",
        "        probability = model[context].get(word, base_probability)\n",
        "        log_probability_sum += math.log(probability)\n",
        "\n",
        "    average_log_probability = log_probability_sum / len(test_ngrams)\n",
        "    perplexity = math.exp(-average_log_probability)\n",
        "\n",
        "    return perplexity\n",
        "\n",
        "def process_text2(training_ngrams, vocabulary, test_ngrams, n_value, smoothing='Laplace', smoothing_factor=1):\n",
        "    \"\"\"\n",
        "    Train the model with the training data and compute the perplexity of the test data.\n",
        "\n",
        "    :param training_ngrams: Sequences of n-grams from the training data, including special tokens.\n",
        "    :param vocabulary: A set derived from the training data, including special tokens.\n",
        "    :param test_ngrams: Sequences of n-grams from the test data, including special tokens.\n",
        "    :param n_value: Indicates the n-gram model (e.g., 2 for bigrams, 3 for trigrams).\n",
        "    :param smoothing: Type of smoothing applied.\n",
        "    :param smoothing_factor: Smoothing factor during model initialization.\n",
        "    \"\"\"\n",
        "    # Initialize and train the model\n",
        "    model, base_probability = train_model2(training_ngrams, vocabulary, n_value, smoothing, smoothing_factor)\n",
        "\n",
        "    # Compute and print the perplexity of the test data\n",
        "    perplexity = compute_perplexity2(test_ngrams, model, base_probability)\n",
        "    print(f\"Perplexity: {perplexity} of {'Original text with Bigrams' if n_value == 2 else 'Original text with Trigrams'} and k={smoothing_factor} \")\n",
        "\n",
        "# Example Usage\n",
        "# Note: Only main function was changed to call the methods properly, and print function in process_text2\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    train0new = preprocess_text2(\"<BOS>\", \"<EOS>\", \"<UNK>\", original_train, vocabulary_original, bigr)\n",
        "    test0new = preprocess_text2(\"<BOS>\", \"<EOS>\", \"<UNK>\", original_test, vocabulary_original, bigr)\n",
        "\n",
        "    train1new = preprocess_text2(\"<BOS>\", \"<EOS>\", \"<UNK>\", original_train, vocabulary_original, trigr)\n",
        "    test1new = preprocess_text2(\"<BOS>\", \"<EOS>\", \"<UNK>\", original_test, vocabulary_original, trigr)\n",
        "\n",
        "\n",
        "    bigram_model_original_ka2 = process_text2(train0new , vocabulary_original, test0new , bigr,'Laplace', ka)\n",
        "\n",
        "    bigram_model_original_kb2 = process_text2(train0new , vocabulary_original, test0new , bigr,'Laplace', kb)\n",
        "\n",
        "    trigram_model_original_ka2 = process_text2(train1new , vocabulary_original, test1new , trigr,'Laplace',ka)\n",
        "\n",
        "    trigram_model_original_kb2 = process_text2(train1new , vocabulary_original, test1new , trigr ,'Laplace',kb)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUoHqKz9pp6z",
        "outputId": "a6ab0478-b927-45d7-d94f-86e8b486e26d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 437.91990460192983 of Original text with Bigrams and k=1 \n",
            "Perplexity: 296.8463590843814 of Original text with Bigrams and k=0.01 \n",
            "Perplexity: 2228.697161847104 of Original text with Trigrams and k=1 \n",
            "Perplexity: 5519.48279959992 of Original text with Trigrams and k=0.01 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comments on chat gpt code\n",
        "\n",
        "The results show similar trend to the original code. Smoothing factor k=0.01 shows better performance than k=1 on bigrams. Also, the results for the bigrams differ only 50-100 from initial results which is not substantial difference. Regarding the trigrams, the scores are significantly worse, since they are 500-5000 units higher. This presumably happens because the perplexity in this implementation is calculated in a more straight-forward way."
      ],
      "metadata": {
        "id": "vZS8kMg-3qmz"
      }
    }
  ]
}